---
layout: default
title:  "[Java][JVM][Profiling][G1GC] Full GC every hour after deploying new application version"
date:   2021-05-16 09:51:30 +0100
---

# [Java][JVM][Profiling] _Full GC_ every hour after deploying new application version
## The _Full GC_ count monitoring

Current count of _Full GC_, if you are using **G1GC**, can be fetched by _JMX_. It is in the _MBean_ with name: 
```java.lang:type=GarbageCollector,name=G1 Old Generation``` at the attribute ```CollectionCount```:

![alt text](/assets/fullgc/2.png "mbean")

## The _Full GC_ count change

Before new version deployment, the profiled application has around **one** _Full GC_ per month. After the next deployment, that count started increasing **every
hour**. I have seen such a behaviour before, and it was related to _RMI_ forcing _Full GC_, you can read about it 
[here](https://plumbr.io/blog/garbage-collection/rmi-enforcing-full-gc-to-run-hourly).

## _GC logs_

```shell
$ cat gc.log | grep "Pause Full"
00:22:52 GC(180) Pause Full (System.gc())
00:22:52 GC(180) Pause Full (System.gc()) 973M->121M(3072M) 276.805ms
01:22:52 GC(181) Pause Full (System.gc())
01:22:53 GC(181) Pause Full (System.gc()) 941M->121M(3072M) 282.143ms
02:59:30 GC(186) Pause Full (System.gc())
02:59:30 GC(186) Pause Full (System.gc()) 989M->126M(3072M) 337.942ms
03:59:30 GC(187) Pause Full (System.gc())
03:59:30 GC(187) Pause Full (System.gc()) 990M->122M(3072M) 337.337ms
05:12:49 GC(197) Pause Full (System.gc())
05:12:49 GC(197) Pause Full (System.gc()) 1141M->121M(3072M) 288.840ms
06:12:49 GC(198) Pause Full (System.gc())
06:12:49 GC(198) Pause Full (System.gc()) 992M->121M(3072M) 324.044ms
07:12:49 GC(199) Pause Full (System.gc())
07:12:50 GC(199) Pause Full (System.gc()) 946M->120M(3072M) 329.932ms
08:12:50 GC(200) Pause Full (System.gc())
08:12:50 GC(200) Pause Full (System.gc()) 942M->120M(3072M) 298.876ms
09:12:50 GC(201) Pause Full (System.gc())
09:12:50 GC(201) Pause Full (System.gc()) 989M->120M(3072M) 325.719ms
10:12:50 GC(202) Pause Full (System.gc())
10:12:51 GC(202) Pause Full (System.gc()) 992M->121M(3072M) 280.892ms
```

In the _GC logs_ we can see that the reason for _Full GC_ is **explicit invocation** of the method ```java.lang.System.gc()```. A little warning here, that method is
just a wrapper for next method:

```java
public static void gc() {
    Runtime.getRuntime().gc();
}
```

## Finding stacktrace that uses that method

If we want to find out who is calling that method, we can use _Async-profiler_ at the **method mode**:

```shell
./profiler.sh -e java/lang/System.gc -o collapsed -f gc.txt -d 3600 jps
```

Even better is to find usages of invocation of method in ```Runtime``` class. That method is declared as ```native```:

```java
public native void gc();
```

To run _Async-profiler_ for **native method** we need to use its name from _JVM_ source code:

```c
JNIEXPORT void JNICALL
Java_java_lang_Runtime_gc(JNIEnv *env, jobject this)
{
    JVM_GC();
}
```

So we need to execute:
```shell
./profiler.sh -e Java_java_lang_Runtime_gc -o collapsed -f gc.txt -d 3600 jps
```

What I saw in the result:

![alt text](/assets/fullgc/3.png "flame")

At the bottom we can see that the thread executing that method is _RMI GC Daemon_. 

## Why was that thread started in the new version?

The easiest way to find out what part of code is to put a _conditional breakpoint_ in IDE. That breakpoint is set in the ```Thread``` class constructor:

```java
private Thread(ThreadGroup g, Runnable target, String name,
               long stackSize, AccessControlContext acc,
               boolean inheritThreadLocals) {
    ...    
}
```

The condition is:

![alt text](/assets/fullgc/4.png "breakpoint")

After starting the application at the localhost, it stopped at that breakpoint. The debugger result:

![alt text](/assets/fullgc/5.png "breakpoint")

At the bottom of the stacktrace we can see that the thread is run because ```org.apache.activemq.broker.jmx.ManagementContext$1``` starts it. I confirmed 
that _ActiveMQ_ was added to the classpath in the new version, so that addition is the reason behind those _FullGCs_.

## Solution

The problem is very old, there are multiple solutions you can find, the one I've decided to go with is to add the **JVM parameter** 
```-XX:+ExplicitGCInvokesConcurrent```. With that flag the explicit invocation of ```System.gc()``` doesn't run _FullGC_ but starts a
**concurrent _G1GC_ phase**. Other possible solutions:
* Disabling explicit ```System.gc()``` with ```-XX:+DisableExplicitGC``` - that may be dangerous if you are using _NIO byte buffers_, you can read about it
[here](https://stackoverflow.com/questions/32912702/impact-of-setting-xxdisableexplicitgc-when-nio-direct-buffers-are-used)
* Changing the _Full GC_ intervals with flags ```-Dsun.rmi.dgc.client.gcInterval=...  -Dsun.rmi.dgc.server.gcInterval=...``` 
