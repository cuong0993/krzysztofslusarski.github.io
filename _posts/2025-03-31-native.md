---
layout: default
title: "[Java][Profiling] Java/JVM native memory \"leaks\""
date: 2025-03-31 02:51:30 +0100
---

# [Java][Profiling] Java/JVM native memory "leaks"

In this article I would like to tackle native memory "leaks" problem. Usually such a problem
starts with some monitoring system that alerts about high memory (RAM) utilization. We start the investigation and
we see that our Java process have eaten 8GB of RAM with `Xmx/Xms` set to 4GB. How is that possible?

Well, heap is not the only memory needed for Java application to run. Other pretty obvious memory areas are:

* **Threads** - each thread needs some native memory for its data
* **Metaspace** - for our class definitions
* **Code cache/code heap** - for JIT compiler output
* ...

## NMT

JVM has dedicated feature to track where the memory was needed. It's called "Native memory tracking" (NMT) and it's
disabled by default. There are two modes of NMT:

* **Summary** - this mode tracks the number of bytes allocated by JVM in each of NMT category
* **Detail** - this one gathers same data as summary + keeps track which part of the code needed that memory

There are many rumors and misconceptions around NMT overhead. In the
[Oracle documentation](https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/tooldescr007.html){:target="_blank"}
you can find:

> Enabling NMT will result in 5-10 percent JVM performance drop and memory usage for NMT adds 2 words to all malloc
> memory as malloc header. NMT memory usage is also tracked by NMT.

Confusing part of this quote is the fact that it's written in **detail** mode explanation, and I cannot tell if
it's generic statement for both modes or just the latter.

Much better explanation you can find in [this Stack Overflow response](https://stackoverflow.com/a/73167790){:target="_blank"}
by Andrei Pangin:

> The overhead of Native Memory Tracking obviously depends on how often the application allocates native memory. Usually, this is not something too frequent in a Java application,
> but cases may differ. Since you've already tried and didn't notice performance difference, your application is apparently not an exception.
>
> In the `summary` mode, Native Memory Tracking roughly does the following things:
>
> * increases every `malloc` request in the JVM by 2 machine words (16 bytes);
> * records the allocation size and flags in these 2 words;
> * atomically increments (or decrements on `free`) the counter corresponding to the given memory type;
> * besides `malloc` and `free`, it also handles changes in virtual memory reservation and allocations of new arenas, but these are even less frequent than `malloc/free` calls.
>
> So, to me, the overhead is quite small; 5-10% is definitely a large overestimation (the numbers would make sense for `detail` mode which collects and stores stack traces, which is expensive, but
> `summary` doesn't do that).
> When many threads concurrently allocate/free native memory, the update of an atomic counter could become a bottleneck, but again, that's more like an extreme case. In short, if you measured a real
> application and didn't notice any degradation, you're likely safe to enable NMT `summary` in production.

The most important question is: can I run NMT in production? From my experience: most likely yes, in **summary** mode. We also need to mind circumstances when such a feature is needed - when we trace
strange memory consumption. It's really hard job, so we should use all the tools we can to find root cause of the problem.

### Usage - summary

To enable NMT in **summary** mode you need to start JVM with `-XX:NativeMemoryTracking=summary`. After that whenever we want to understand why JVM allocated some native memory we can run:

```shell
jcmd <pid> VM.native_memory summary scale=MB
```

Sample output:

```
Native Memory Tracking:

(Omitting categories weighting less than 1MB)

Total: reserved=2340MB, committed=1237MB
-                 Java Heap (reserved=1024MB, committed=1024MB)
                            (mmap: reserved=1024MB, committed=1024MB) 
 
-                     Class (reserved=1025MB, committed=11MB)
                            (classes #16326)
                            (  instance classes #15180, array classes #1146)
                            (malloc=1MB #31555) 
                            (mmap: reserved=1024MB, committed=10MB) 
                            (  Metadata:   )
                            (    reserved=64MB, committed=56MB)
                            (    used=55MB)
                            (    waste=0MB =0,63%)
                            (  Class space:)
                            (    reserved=1024MB, committed=10MB)
                            (    used=10MB)
                            (    waste=0MB =2,88%)
 
-                    Thread (reserved=50MB, committed=4MB)
                            (thread #51)
                            (stack: reserved=50MB, committed=4MB)
 
-                      Code (reserved=49MB, committed=15MB)
                            (malloc=1MB #7383) 
                            (mmap: reserved=48MB, committed=14MB) 
 
-                        GC (reserved=90MB, committed=90MB)
                            (malloc=19MB #9300) 
                            (mmap: reserved=70MB, committed=70MB) 
 
-                    Symbol (reserved=17MB, committed=17MB)
                            (malloc=15MB #401954) 
                            (arena=1MB #1)
 
-    Native Memory Tracking (reserved=7MB, committed=7MB)
                            (tracking overhead=7MB)
 
-        Shared class space (reserved=12MB, committed=12MB)
                            (mmap: reserved=12MB, committed=12MB) 
 
-               Arena Chunk (reserved=1MB, committed=1MB)
                            (malloc=1MB) 
 
-                 Metaspace (reserved=64MB, committed=56MB)
                            (mmap: reserved=64MB, committed=56MB) 
```

### Usage - diff

If we want to understand which NMT category is increasing we don't need to compare two outputs like the one above. NMT has built-in diff operation. First we need to set baseline with:

```shell
jcmd <pid> VM.native_memory baseline
```

After some time (when we see that more memory was consumed by Java process) we can run:

```shell
jcmd <pid> VM.native_memory summary.diff scale=MB
```

Sample output:

```
Native Memory Tracking:

(Omitting categories weighting less than 1MB)

Total: reserved=3350MB +1004MB, committed=2257MB +1013MB

-                 Java Heap (reserved=1024MB, committed=1024MB)
                            (mmap: reserved=1024MB, committed=1024MB)
 
-                     Class (reserved=1026MB, committed=12MB)
                            (classes #16935 +7)
                            (  instance classes #15732 +4, array classes #1203 +3)
                            (malloc=2MB #34724 +2040)
                            (mmap: reserved=1024MB, committed=10MB)
                           : (  Metadata)
                            (    reserved=64MB, committed=58MB)
                            (    used=57MB)
                            (    waste=0MB =0,58%)
                           : (  Class space)
                            (    reserved=1024MB, committed=10MB)
                            (    used=10MB)
                            (    waste=0MB =2,82%)
 
-                    Thread (reserved=48MB -4MB, committed=4MB)
                            (thread #0)
                            (stack: reserved=48MB -4MB, committed=4MB)
 
-                      Code (reserved=50MB, committed=21MB +6MB)
                            (malloc=1MB #10044 +2426)
                            (mmap: reserved=48MB, committed=20MB +6MB)
 
-                        GC (reserved=90MB, committed=90MB)
                            (malloc=19MB #13489 +3620)
                            (mmap: reserved=70MB, committed=70MB)
 
-                     Other (reserved=1011MB +1007MB, committed=1011MB +1007MB)
                            (malloc=1011MB +1007MB #1025 +1008)
 
-                    Symbol (reserved=17MB, committed=17MB)
                            (malloc=16MB #412735 +275)
                            (arena=1MB #1)
 
-    Native Memory Tracking (reserved=7MB, committed=7MB)
                            (tracking overhead=7MB)
 
-        Shared class space (reserved=12MB, committed=12MB)
                            (mmap: reserved=12MB, committed=12MB)
 
-                 Metaspace (reserved=64MB, committed=58MB)
                            (mmap: reserved=64MB, committed=58MB)
```

First we can look at information in line starting with _Total_ where we can see _+1013MB_. It means that our JVM allocated almost **1GB** of native memory since we run `baseline` command.
When we browse all the categories we can see this one:

```
-                     Other (reserved=1011MB +1007MB, committed=1011MB +1007MB)
                            (malloc=1011MB +1007MB #1025 +1008)
```

### NMT - categories

Let's now go through those categories and try to understand what we can do if memory consumption is increasing there.

* `Java Heap` - Increase in that category means that your initial or minimal heap size is different then max one and heap simply grows.
* `Class` - New classes where loaded in the runtime. Depending on the volume of consumed memory it can be natural behavior of the application. Many libraries define/load classes in the runtime.
   If the increase here is abnormal you can try to profile where new classes are loaded with [Async-profiler](../../../2022/12/12/async-manual.html#methods-classes){:target="_blank"}.
* `Thread` - In most configuration every Java thread can use up to **1MB** (`-Xxx`). Increasing consumption in that category usually means:
  * that existing threads needed more memory - for example it reached code with deep stack
  * new threads were created - you can find places that create new threads with [Async-profiler](../../../2022/12/12/async-manual.html#methods-thread){:target="_blank"}.
* `Code` - This is a space for JIT, it can grow up to **~240MB** (`-XX:ReservedCodeCacheSize`). Increasing consumption here is natural. The longer our JVM is running the more methods are compiled 
   by JIT.
* `Other` - Here you can find native memory allocated with `ByteBuffer.allocateDirect`. If you want to understand why this category grows you can run Async-profiler in method mode to find
   invocations of the `allocateDirect` method, or you can analyze the **heap dump**.

Those are the most common scenarios I approached in my carrier. Mind that JVM can also have issues in its code that results in large memory consumption like 
[JDK-8240723](https://bugs.openjdk.org/browse/JDK-8240723){:target="_blank"}.

## NMT vs RSS/PSS vs total memory allocated in the system

To move on I want to point out the difference between three metrics:
* **NMT committed memory** - how much memory JVM used, counted by JVM internals.
* **RSS/PSS** - how much memory Java process used from OS perspective.
* **Total memory allocated in the system** - this one is straightforward, it counts not only Java process but everything system-wide.

Today many of new applications written in Java is run in Docker container so we would expect those three values to be the same. Well, reality is a bit different.  

### Native libs

When you run your Java code you are allowed to invoke native code with JNI (and other mechanisms, but that's irrelevant). The code inside external native library is out of JVM's control.
That means that if our native code does `malloc`, JVM doesn't know about that. This basically means that NMT **cannot trace such an allocation**. Remember that you don't need to invoke JNI by
yourself, your dependencies can do it for you. Lot of libraries does that. if your code or yours dependency code has native memory leak done inside JNI section RSS/PSS can grow while NMT commited
memory will stay the same. 

To track down native memory leak (no matter if it is inside JVM or native libs) we need to find allocated memory that hasn't been freed. For many years there was tool that could do it and give you
Java stacktraces. In the past you could use [Async-profiler](../../../2022/12/12/async-manual.html#perf-pf){:target="_blank"} and trace:
* `page-faults`
* `malloc` invocations
* `mprotect` invocations

but all we could get from it was where memory was allocated. We didn't know which parts of allocated memory were freed later. For all those years my approach to finding native memory leaks was as 
follows:

* Check **JMX metrics** (either in Grafana/Zabbix/... or by manual JMX connection) to see what JVM reports for classloading and direct buffer allocations.
* If there was no visible leak - **turn on NMT** in _summary_ mode and pray that the answer is there.
* If there was no visible leak in NMT - **profile** for `page-faults` and filter manually evey allocation that is tracked by NMT
* For every native allocation found while RSS/PSS was increasing:
  * Filter manually evey allocation that is tracked by NMT
  * Run such a code in isolation.
  * Check if RSS/PSS increases.

Current "microservices" can have easily tens (and even more than a honoured) different places where native memory is allocated. That was really time-consuming work, and from time to time there was
no native memory leak after all - we wil get to that later.

Thankfully those dark times are over. There is new mode in Async-profiler called `nativemem`. It keep tracks of allocations and frees and can show us which memory hasn't been freed during profiling.
That mode literally can save you hours of your life with investigation. While writing this article this mode is not included in any release (current last release is **3.0**), so we need to use
[nightly builds](https://github.com/async-profiler/async-profiler/releases/tag/nightly){:target="_blank"} for now. 

### Malloc