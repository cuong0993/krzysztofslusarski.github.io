---
layout: default
title:  "[Java][Performance] Thread-per-core architecture - basics"
date:   2022-11-24 02:51:30 +0100
---

# [Java][Performance] Thread-per-core architecture - basics

## History of the hardware

Let's go back to the past. Fifteen years ago hardware world was like this:

- The **CPU** was faster every generation
- The **memory** was cheaper every year
- The **hard drives** were extremely slow
- The **network** wasn't so great in terms of bandwidth and reliability

Our **bottlenecks** used to be hard drives and network. On top of that we started to build systems in staged event-driven architecture (**SEDA**).

## SEDA

Let's consider that we are creating new company that is going to create some enterprise application. Let's assume that we need:

- Business analysts
- Java programmers
- Angular programmers

![alt text](/assets/tpc/seda-1.png "seda")

Our workflow is pretty straightforward. Business analyst create a document that is passed to Java programmer who creates new REST API that
is consumed by Angular part of the software. To make that workflow running we need some system where we can trace the progress. 
Some kind of project management tool, let's call that system "**J**" (visualized as green lines above).

Our company gets bigger, we want to do multiple projects at the same time. How should we scale? 

Should we scale every each group linearly?

![alt text](/assets/tpc/seda-2.png "seda")

Or maybe we need more Java programmers than others?

![alt text](/assets/tpc/seda-3.png "seda")

Maybe sometimes we will need more Java programmers, and sometimes we will need more Angular ones. Hard to predict the most optimal layout.
                                    
Think about each group of people as a **thread pool**. **SEDA** creates separate thread pools, each pool is designed to do one part of a job and
to pass the result to the next pool. Scaling thread pools in large application is not a trivial task. Especially if load to the application differs during
work day. 

Ok, let's face another problem. We have only one floor in our office. We cannot put there infinite number of employees. We need to scale
that too. So if we want to grow more, we need to lend another floor. Sometimes we need a consensus between floors regarding some decision.
How we can get one? Let's hire a mid-level manager for each floor. If we need some consensus the managers need to agree on it.

![alt text](/assets/tpc/seda-4.png "seda")

There are two problems with that company architecture:

- we scaled a number of each employee's group, but we didn't scale "J", so this may become our bottleneck now
- managers have a fixed capacity, so they also may become a bottleneck 

In my story you can think about floors as **cores** in your **CPU**. Managers can be for example your MESI protocol, that need to 
guarantee to you that your data in memory are coherent. All of that comes down to 
[universal scalability law](https://wso2.com/blog/research/scalability-modeling-using-universal-scalability-law/){:target="_blank"}.
Your architecture cannot scale
linearly as long as you have:

- **serialization points** (like "J")
- **interprocess communications** (like managers)

**Side note:** a purpose of this article is not you to remember that "J" and managers slows down your company (even if you know it's true).

## TPC

Since we cannot scale our company using SEDA let's try to figure out something different. What if instead of hiring few groups of people
we could hire someone who can do all the job. Yes, here it comes:

![alt text](/assets/tpc/full.png "full")

Let's assume for a purpose of that article that the full stack developer can do all the job very good (I know it's a really strong assumption).

We want to do four project simultaneously? Not a problem, we hire four of those, and we assign each project to each worker:

![alt text](/assets/tpc/tpc-1.png "tpc")

We need to double the throughput of our company? Not a problem, we hire four full stacks more.

![alt text](/assets/tpc/tpc-2.png "tpc")

To make the full stack work efficient we assign one floor to one full stack. That's basically the TPC architecture. We want to have one
thread per core that can do all the job to handle requests.

## Current state of the hardware

I already wrote about history, how it looks in 2022:

- The **CPU** is not getting more GHz each year, it gets more cores
- The **memory** is still expensive comparing to SSD drives
- The **NVME SSDs** are very fast with decent latency
- The **network** is much faster than it used to be, and also more reliable

In the past we used blocking IO APIs to connect to our bottlenecks (HD and net). SEDA was a good desing for that. If one thread
was blocked on IO it was descheduled from CPU by OS and other thread could take that resource. OSs created also a
[page cache](https://en.wikipedia.org/wiki/Page_cache){:target="_blank"}, which reduces disk IOs.

But the world has changed. NVME SSDs and network are so fast that it's hard to call them a bottleneck. We are not forced to 
use blocking APIs for them too, we have asynchronous ones already provided. If HD and net are no longer a bottleneck, where is it now?
Of course, it depends on the application, but it can easily be your CPU. 

SEDA didn't care about CPU usage, each descheduling of blocked thread was a **context switch** which usually comes with some 
**cache misses**. We didn't care, because CPU was not the bottleneck. Well, it can be now.

## TPC on modern hardware

If you can design application in TPC architecture what you get out-of-the-box is:

- reduced amount of **context switches**
- much less **L1/L2/LLC misses**
- much less **remote RAM** accesses between **NUMA nodes**
- **easier code** to maintain - you don't care about concurrence since one thread has access to it's memory
- easy way to do **no litter** code in Java, since you can create thread-local object pools

In C++ world there is already a framework that helps you with creation of application in that design called 
[Seastar](https://seastar.io/){:target="_blank"}.

## TPC in Java

